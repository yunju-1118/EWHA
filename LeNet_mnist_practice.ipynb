{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP49b7haDoadvOZgdJmrrO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunju-1118/EWHA/blob/main/LeNet_mnist_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vlSQcSvhsh97"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MNIST**"
      ],
      "metadata": {
        "id": "yPA1-1Yf3GNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## data loader\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path,train=True,transform=transform,download=True)\n",
        "test_data = MNIST(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=4)\n",
        "\n",
        "input_shape = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUf9Zj_ksung",
        "outputId": "d64c9e1f-ae81-4763-f6ce-56cd98a42644"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 497kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.59MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.35MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## model definition\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #### HERE ####\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2)\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=16*5*5, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84, out_features=output_shape)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        #### HERE ####\n",
        "#       print(x.shape)# 100, 1, 28, 28\n",
        "        hidden = F.leaky_relu(self.conv1(x))\n",
        "#       print(hidden.shape)# 100, 6, 28, 28\n",
        "        hidden = self.pool1(hidden)\n",
        "#       print(hidden.shape)# 100, 6, 14, 14\n",
        "        hidden = F.leaky_relu(self.conv2(hidden))\n",
        "#       print(hidden.shape)# 100, 16, 10, 10\n",
        "        hidden = self.pool2(hidden)\n",
        "#       print(hidden.shape)# 100, 16, 5, 5\n",
        "        hidden = self.flatten(hidden)\n",
        "#       print(hidden.shape)# 100, 400 => clf의 목적에 부합하는 이미지의 vector 표현형\n",
        "        hidden = F.leaky_relu(self.fc1(hidden))\n",
        "#       print(hidden.shape)# 100, 120\n",
        "        hidden = F.leaky_relu(self.fc2(hidden))\n",
        "#       print(hidden.shape)# 100, 84\n",
        "        output = self.fc3(hidden)\n",
        "#       print(output.shape)# 100, 10\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "EYb56ClKs0G6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conv2d( )**:\n",
        "\n",
        "in_channels: input, out_channels: output, kernel_size, padding\n",
        "\n",
        "gray-scale img => input channel => 1\n",
        "\n",
        "그 외\n",
        "\n",
        "\n",
        "- pooling layer는 재사용 가능 -> 학습을 하지 않기 때문 같은 이유로 drop out 등도 가능\n",
        "\n",
        "- convolution layer의 경우, parameter가 모두 같아도 재사용 불가능 -> 학습\n"
      ],
      "metadata": {
        "id": "HRVjFI8iwT7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "BvT9dr5P1C5a"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # train\n",
        "    #### HERE ####\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        #### HERE ####\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        # backward computation\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=-1)\n",
        "        count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "    # eval\n",
        "    #### HERE ####\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "\n",
        "            #### HERE ####\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            y_est = model.forward(x)\n",
        "            cost = loss(y_est, y)\n",
        "\n",
        "            total_loss += cost.item()*len(x)\n",
        "\n",
        "            pred = torch.argmax(y_est, dim=-1)\n",
        "            count += (pred==y).sum().item()\n",
        "\n",
        "        acc = count/len(test_data)\n",
        "        ave_loss = total_loss/len(test_data)\n",
        "\n",
        "        test_loss_list.append(ave_loss)\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(\"Epoch %d Test: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():        # 각 layer마다 존재하는 parameter 개수의 합\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "id": "xjzGK9GS3SbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CIFAR100**"
      ],
      "metadata": {
        "id": "AZlO7fv43Lv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LeNet**"
      ],
      "metadata": {
        "id": "TNHn5dED_GOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## data loader\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
        "                                                     std = [0.2675, 0.2565, 0.2761])]) # 각각의 channel에 대\n",
        "\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data,batch_size=batch_size,shuffle=False,num_workers=4)\n",
        "\n",
        "input_shape = train_data[0][0] #.shape # 3*32*32\n",
        "output_shape = len(train_data.classes)  # 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa3bfe7-c426-4355-917c-2dc75004e5b2",
        "id": "-dBCGS5-3PAQ"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.2MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## model definition\n",
        "\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # classifier와 feature extractor를 나눠서 구현\n",
        "        self.fe = nn.Sequential(# 100, 3, 32, 32\n",
        "            nn.Conv2d(in_channels=3, out_channels=9, kernel_size=3, padding=1), # 100, 9, 32, 32\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=9, out_channels=18, kernel_size=5, padding=2), # 100, 18, 32, 32\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2), # 100, 18, 16, 16\n",
        "            nn.Conv2d(in_channels=18, out_channels=32, kernel_size=4, stride=2, padding=1), # 100, 32, 8, 8\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        ) # 100, 32, 4, 4\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512,256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        hidden = self.fe(x)\n",
        "        hidden = self.flatten(hidden)\n",
        "        output = self.fc(hidden)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8ECohH983PAQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "BZRQ54VV3PAQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LeNet().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
        "\n",
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "\n",
        "    # train\n",
        "    #### HERE ####\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "\n",
        "        #### HERE ####\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_est = model.forward(x)\n",
        "        cost = loss(y_est, y)\n",
        "\n",
        "        # backward computation\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=-1)\n",
        "        count += (pred == y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "    # eval\n",
        "    #### HERE ####\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x, y) in enumerate(test_loader):\n",
        "\n",
        "            #### HERE ####\n",
        "            x,y = x.to(device), y.to(device)\n",
        "            y_est = model.forward(x)\n",
        "            cost = loss(y_est, y)\n",
        "\n",
        "            total_loss += cost.item()*len(x)\n",
        "\n",
        "            pred = torch.argmax(y_est, dim=-1)\n",
        "            count += (pred==y).sum().item()\n",
        "\n",
        "        acc = count/len(test_data)\n",
        "        ave_loss = total_loss/len(test_data)\n",
        "\n",
        "        test_loss_list.append(ave_loss)\n",
        "\n",
        "        if i % 1 == 0:\n",
        "            print(\"Epoch %d Test: %.3f / %.3f\"%(i,ave_loss,acc))\n",
        "\n",
        "\n",
        "print()\n",
        "num_parameter = 0\n",
        "for parameter in model.parameters():        # 각 layer마다 존재하는 parameter 개수의 합\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b077f4c2-98df-46a6-9d85-997b65d65d6e",
        "id": "9E8QAb6N3PAR"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train: 0.000 / 0.083\n",
            "Epoch 0 Test: 3.638 / 0.147\n",
            "\n",
            "Epoch 1 Train: 0.000 / 0.145\n",
            "Epoch 1 Test: 3.380 / 0.191\n",
            "\n",
            "Epoch 2 Train: 0.000 / 0.182\n",
            "Epoch 2 Test: 3.149 / 0.238\n",
            "\n",
            "Epoch 3 Train: 0.000 / 0.212\n",
            "Epoch 3 Test: 3.051 / 0.256\n",
            "\n",
            "Epoch 4 Train: 0.000 / 0.231\n",
            "Epoch 4 Test: 2.955 / 0.282\n",
            "\n",
            "Epoch 5 Train: 0.000 / 0.247\n",
            "Epoch 5 Test: 2.911 / 0.296\n",
            "\n",
            "Epoch 6 Train: 0.000 / 0.259\n",
            "Epoch 6 Test: 2.833 / 0.303\n",
            "\n",
            "Epoch 7 Train: 0.000 / 0.273\n",
            "Epoch 7 Test: 2.778 / 0.309\n",
            "\n",
            "Epoch 8 Train: 0.000 / 0.281\n",
            "Epoch 8 Test: 2.754 / 0.317\n",
            "\n",
            "Epoch 9 Train: 0.000 / 0.291\n",
            "Epoch 9 Test: 2.693 / 0.329\n",
            "\n",
            "Epoch 10 Train: 0.000 / 0.298\n",
            "Epoch 10 Test: 2.676 / 0.336\n",
            "\n",
            "Epoch 11 Train: 0.000 / 0.310\n",
            "Epoch 11 Test: 2.651 / 0.335\n",
            "\n",
            "Epoch 12 Train: 0.000 / 0.317\n",
            "Epoch 12 Test: 2.622 / 0.347\n",
            "\n",
            "Epoch 13 Train: 0.000 / 0.322\n",
            "Epoch 13 Test: 2.599 / 0.351\n",
            "\n",
            "Epoch 14 Train: 0.000 / 0.332\n",
            "Epoch 14 Test: 2.570 / 0.356\n",
            "\n",
            "Epoch 15 Train: 0.000 / 0.337\n",
            "Epoch 15 Test: 2.519 / 0.362\n",
            "\n",
            "Epoch 16 Train: 0.000 / 0.340\n",
            "Epoch 16 Test: 2.524 / 0.367\n",
            "\n",
            "Epoch 17 Train: 0.000 / 0.346\n",
            "Epoch 17 Test: 2.503 / 0.375\n",
            "\n",
            "Epoch 18 Train: 0.000 / 0.350\n",
            "Epoch 18 Test: 2.485 / 0.374\n",
            "\n",
            "Epoch 19 Train: 0.000 / 0.358\n",
            "Epoch 19 Test: 2.451 / 0.380\n",
            "\n",
            "Epoch 20 Train: 0.000 / 0.361\n",
            "Epoch 20 Test: 2.452 / 0.383\n",
            "\n",
            "Epoch 21 Train: 0.000 / 0.367\n",
            "Epoch 21 Test: 2.436 / 0.379\n",
            "\n",
            "Epoch 22 Train: 0.000 / 0.368\n",
            "Epoch 22 Test: 2.416 / 0.391\n",
            "\n",
            "Epoch 23 Train: 0.000 / 0.375\n",
            "Epoch 23 Test: 2.410 / 0.386\n",
            "\n",
            "Epoch 24 Train: 0.000 / 0.379\n",
            "Epoch 24 Test: 2.385 / 0.392\n",
            "\n",
            "Epoch 25 Train: 0.000 / 0.382\n",
            "Epoch 25 Test: 2.402 / 0.385\n",
            "\n",
            "Epoch 26 Train: 0.000 / 0.385\n",
            "Epoch 26 Test: 2.376 / 0.393\n",
            "\n",
            "Epoch 27 Train: 0.000 / 0.390\n",
            "Epoch 27 Test: 2.368 / 0.398\n",
            "\n",
            "Epoch 28 Train: 0.000 / 0.396\n",
            "Epoch 28 Test: 2.365 / 0.398\n",
            "\n",
            "Epoch 29 Train: 0.000 / 0.395\n",
            "Epoch 29 Test: 2.354 / 0.402\n",
            "\n",
            "Epoch 30 Train: 0.000 / 0.401\n",
            "Epoch 30 Test: 2.343 / 0.401\n",
            "\n",
            "Epoch 31 Train: 0.000 / 0.400\n",
            "Epoch 31 Test: 2.327 / 0.404\n",
            "\n",
            "Epoch 32 Train: 0.000 / 0.406\n",
            "Epoch 32 Test: 2.339 / 0.402\n",
            "\n",
            "Epoch 33 Train: 0.000 / 0.404\n",
            "Epoch 33 Test: 2.310 / 0.412\n",
            "\n",
            "Epoch 34 Train: 0.000 / 0.408\n",
            "Epoch 34 Test: 2.314 / 0.409\n",
            "\n",
            "Epoch 35 Train: 0.000 / 0.412\n",
            "Epoch 35 Test: 2.305 / 0.413\n",
            "\n",
            "Epoch 36 Train: 0.000 / 0.415\n",
            "Epoch 36 Test: 2.316 / 0.407\n",
            "\n",
            "Epoch 37 Train: 0.000 / 0.418\n",
            "Epoch 37 Test: 2.305 / 0.412\n",
            "\n",
            "Epoch 38 Train: 0.000 / 0.418\n",
            "Epoch 38 Test: 2.301 / 0.413\n",
            "\n",
            "Epoch 39 Train: 0.000 / 0.422\n",
            "Epoch 39 Test: 2.277 / 0.413\n",
            "\n",
            "Epoch 40 Train: 0.000 / 0.421\n",
            "Epoch 40 Test: 2.280 / 0.418\n",
            "\n",
            "Epoch 41 Train: 0.000 / 0.425\n",
            "Epoch 41 Test: 2.278 / 0.414\n",
            "\n",
            "Epoch 42 Train: 0.000 / 0.427\n",
            "Epoch 42 Test: 2.266 / 0.420\n",
            "\n",
            "Epoch 43 Train: 0.000 / 0.431\n",
            "Epoch 43 Test: 2.282 / 0.411\n",
            "\n",
            "Epoch 44 Train: 0.000 / 0.433\n",
            "Epoch 44 Test: 2.271 / 0.414\n",
            "\n",
            "Epoch 45 Train: 0.000 / 0.435\n",
            "Epoch 45 Test: 2.257 / 0.417\n",
            "\n",
            "Epoch 46 Train: 0.000 / 0.434\n",
            "Epoch 46 Test: 2.246 / 0.419\n",
            "\n",
            "Epoch 47 Train: 0.000 / 0.435\n",
            "Epoch 47 Test: 2.267 / 0.414\n",
            "\n",
            "Epoch 48 Train: 0.000 / 0.439\n",
            "Epoch 48 Test: 2.249 / 0.423\n",
            "\n",
            "Epoch 49 Train: 0.000 / 0.442\n",
            "Epoch 49 Test: 2.251 / 0.419\n",
            "\n",
            "Epoch 50 Train: 0.000 / 0.443\n",
            "Epoch 50 Test: 2.238 / 0.423\n",
            "\n",
            "Epoch 51 Train: 0.000 / 0.443\n",
            "Epoch 51 Test: 2.248 / 0.414\n",
            "\n",
            "Epoch 52 Train: 0.000 / 0.447\n",
            "Epoch 52 Test: 2.248 / 0.425\n",
            "\n",
            "Epoch 53 Train: 0.000 / 0.447\n",
            "Epoch 53 Test: 2.252 / 0.419\n",
            "\n",
            "Epoch 54 Train: 0.000 / 0.446\n",
            "Epoch 54 Test: 2.226 / 0.426\n",
            "\n",
            "Epoch 55 Train: 0.000 / 0.449\n",
            "Epoch 55 Test: 2.265 / 0.417\n",
            "\n",
            "Epoch 56 Train: 0.000 / 0.453\n",
            "Epoch 56 Test: 2.231 / 0.429\n",
            "\n",
            "Epoch 57 Train: 0.000 / 0.453\n",
            "Epoch 57 Test: 2.211 / 0.430\n",
            "\n",
            "Epoch 58 Train: 0.000 / 0.459\n",
            "Epoch 58 Test: 2.223 / 0.429\n",
            "\n",
            "Epoch 59 Train: 0.000 / 0.460\n",
            "Epoch 59 Test: 2.216 / 0.428\n",
            "\n",
            "Epoch 60 Train: 0.000 / 0.458\n",
            "Epoch 60 Test: 2.206 / 0.430\n",
            "\n",
            "Epoch 61 Train: 0.000 / 0.461\n",
            "Epoch 61 Test: 2.202 / 0.431\n",
            "\n",
            "Epoch 62 Train: 0.000 / 0.465\n",
            "Epoch 62 Test: 2.193 / 0.438\n",
            "\n",
            "Epoch 63 Train: 0.000 / 0.462\n",
            "Epoch 63 Test: 2.197 / 0.437\n",
            "\n",
            "Epoch 64 Train: 0.000 / 0.466\n",
            "Epoch 64 Test: 2.213 / 0.434\n",
            "\n",
            "Epoch 65 Train: 0.000 / 0.467\n",
            "Epoch 65 Test: 2.172 / 0.440\n",
            "\n",
            "Epoch 66 Train: 0.000 / 0.466\n",
            "Epoch 66 Test: 2.203 / 0.426\n",
            "\n",
            "Epoch 67 Train: 0.000 / 0.467\n",
            "Epoch 67 Test: 2.190 / 0.431\n",
            "\n",
            "Epoch 68 Train: 0.000 / 0.472\n",
            "Epoch 68 Test: 2.187 / 0.434\n",
            "\n",
            "Epoch 69 Train: 0.000 / 0.467\n",
            "Epoch 69 Test: 2.189 / 0.435\n",
            "\n",
            "Epoch 70 Train: 0.000 / 0.471\n",
            "Epoch 70 Test: 2.184 / 0.434\n",
            "\n",
            "Epoch 71 Train: 0.000 / 0.472\n",
            "Epoch 71 Test: 2.183 / 0.435\n",
            "\n",
            "Epoch 72 Train: 0.000 / 0.470\n",
            "Epoch 72 Test: 2.185 / 0.431\n",
            "\n",
            "Epoch 73 Train: 0.000 / 0.472\n",
            "Epoch 73 Test: 2.178 / 0.431\n",
            "\n",
            "Epoch 74 Train: 0.000 / 0.477\n",
            "Epoch 74 Test: 2.185 / 0.432\n",
            "\n",
            "Epoch 75 Train: 0.000 / 0.475\n",
            "Epoch 75 Test: 2.179 / 0.438\n",
            "\n",
            "Epoch 76 Train: 0.000 / 0.479\n",
            "Epoch 76 Test: 2.182 / 0.431\n",
            "\n",
            "Epoch 77 Train: 0.000 / 0.481\n",
            "Epoch 77 Test: 2.184 / 0.433\n",
            "\n",
            "Epoch 78 Train: 0.000 / 0.480\n",
            "Epoch 78 Test: 2.174 / 0.437\n",
            "\n",
            "Epoch 79 Train: 0.000 / 0.479\n",
            "Epoch 79 Test: 2.159 / 0.439\n",
            "\n",
            "Epoch 80 Train: 0.000 / 0.481\n",
            "Epoch 80 Test: 2.171 / 0.438\n",
            "\n",
            "Epoch 81 Train: 0.000 / 0.486\n",
            "Epoch 81 Test: 2.179 / 0.433\n",
            "\n",
            "Epoch 82 Train: 0.000 / 0.486\n",
            "Epoch 82 Test: 2.168 / 0.436\n",
            "\n",
            "Epoch 83 Train: 0.000 / 0.484\n",
            "Epoch 83 Test: 2.178 / 0.441\n",
            "\n",
            "Epoch 84 Train: 0.000 / 0.484\n",
            "Epoch 84 Test: 2.162 / 0.438\n",
            "\n",
            "Epoch 85 Train: 0.000 / 0.488\n",
            "Epoch 85 Test: 2.168 / 0.436\n",
            "\n",
            "Epoch 86 Train: 0.000 / 0.487\n",
            "Epoch 86 Test: 2.177 / 0.435\n",
            "\n",
            "Epoch 87 Train: 0.000 / 0.487\n",
            "Epoch 87 Test: 2.176 / 0.439\n",
            "\n",
            "Epoch 88 Train: 0.000 / 0.490\n",
            "Epoch 88 Test: 2.156 / 0.445\n",
            "\n",
            "Epoch 89 Train: 0.000 / 0.488\n",
            "Epoch 89 Test: 2.162 / 0.438\n",
            "\n",
            "Epoch 90 Train: 0.000 / 0.488\n",
            "Epoch 90 Test: 2.161 / 0.444\n",
            "\n",
            "Epoch 91 Train: 0.000 / 0.493\n",
            "Epoch 91 Test: 2.187 / 0.438\n",
            "\n",
            "Epoch 92 Train: 0.000 / 0.493\n",
            "Epoch 92 Test: 2.166 / 0.440\n",
            "\n",
            "Epoch 93 Train: 0.000 / 0.491\n",
            "Epoch 93 Test: 2.171 / 0.435\n",
            "\n",
            "Epoch 94 Train: 0.000 / 0.496\n",
            "Epoch 94 Test: 2.185 / 0.431\n",
            "\n",
            "Epoch 95 Train: 0.000 / 0.494\n",
            "Epoch 95 Test: 2.170 / 0.444\n",
            "\n",
            "Epoch 96 Train: 0.000 / 0.494\n",
            "Epoch 96 Test: 2.175 / 0.433\n",
            "\n",
            "Epoch 97 Train: 0.000 / 0.492\n",
            "Epoch 97 Test: 2.179 / 0.438\n",
            "\n",
            "Epoch 98 Train: 0.000 / 0.496\n",
            "Epoch 98 Test: 2.182 / 0.435\n",
            "\n",
            "Epoch 99 Train: 0.000 / 0.494\n",
            "Epoch 99 Test: 2.160 / 0.442\n",
            "\n",
            "torch.Size([9, 3, 3, 3])\n",
            "torch.Size([9])\n",
            "torch.Size([18, 9, 5, 5])\n",
            "torch.Size([18])\n",
            "torch.Size([32, 18, 4, 4])\n",
            "torch.Size([32])\n",
            "torch.Size([256, 512])\n",
            "torch.Size([256])\n",
            "torch.Size([100, 256])\n",
            "torch.Size([100])\n",
            "170596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fully-Connected layer**"
      ],
      "metadata": {
        "id": "LaojSFPg_Bex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fully-connected softmax classifier를 구현을 해보시길 바랄게요...\n",
        "\n",
        "처음부터 flatten해서 적용해보깅"
      ],
      "metadata": {
        "id": "hXaemI4x9RiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UPvfCzifB3qd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data loader\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
        "                                                     std = [0.2675, 0.2565, 0.2761])])\n",
        "\n",
        "train_data = CIFAR100(root=path,train=True,transform=transform,download=True)\n",
        "test_data = CIFAR100(root=path,train=False,transform=transform,download=True)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "print(train_data)\n",
        "print(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0qlYbKe_K_P",
        "outputId": "c8ad3ac5-b794-4855-ce3a-a5bcc5e07363"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset CIFAR100\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./datasets/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
            "           )\n",
            "Dataset CIFAR100\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./datasets/\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_data[0][0].reshape(-1).shape[0]\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "print(input_shape, output_shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StG2qBFACtpg",
        "outputId": "09d14669-fef6-4f8c-8876-f53080aef5bd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3072 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Model Definition\n",
        "class SoftmaxClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Flatten(), # 3*32*32 = 3072\n",
        "            nn.Linear(input_shape, 1024), # 100*1024\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(256,output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "SsY5E67VC8kj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps:0\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBLbK8ldGWpV",
        "outputId": "d31ff95d-af19-4217-9550-751abbb42879"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SoftmaxClassifier().to(device)\n",
        "loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, weight_decay=1e-3)"
      ],
      "metadata": {
        "id": "CeMivy3pGjyg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 100\n",
        "train_loss_list, test_loss_list = list(), list()\n",
        "\n",
        "for i in range(num_epoch):\n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    for batch_idx, (x,y) in enumerate(train_loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        pre_y_est = model.forward(x)\n",
        "        cost = loss(pre_y_est, y)\n",
        "        y_est = F.softmax(pre_y_est, dim=-1)\n",
        "\n",
        "        total_loss += cost.item()*len(x)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(y_est, dim=-1)\n",
        "        count += (pred==y).sum().item()\n",
        "\n",
        "    acc = count/len(train_data)\n",
        "    ave_loss = total_loss/len(train_data)\n",
        "\n",
        "    train_loss_list.append(ave_loss)\n",
        "\n",
        "    if i % 1 == 0:\n",
        "        print(\"\\nEpoch %d Train: %.3f / %.3f\"%(i,ave_loss, acc))\n",
        "\n",
        "    # test\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (x,y) in enumerate(test_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            pre_y_est = model.forward(x)\n",
        "            cost = loss(pre_y_est, y)\n",
        "            y_est = F.softmax(pre_y_est, dim=-1)\n",
        "\n",
        "            total_loss += cost.item()*len(x)\n",
        "\n",
        "            pred = torch.argmax(y_est, dim=-1)\n",
        "            count += (pred==y).sum().item()\n",
        "\n",
        "        acc = count/len(test_data)\n",
        "        ave_loss = total_loss/len(test_data)\n",
        "\n",
        "        test_loss_list.append(ave_loss)\n",
        "\n",
        "        if i% 1 == 0:\n",
        "            print(\"Epoch %d Test: %.3f / %.3f\"%(i,ave_loss,acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4fnJAvyGjU0",
        "outputId": "1667ecb3-3035-41d3-e245-a921ad27d42c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 0 Train: 4.600 / 0.013\n",
            "Epoch 0 Test: 4.596 / 0.020\n",
            "\n",
            "Epoch 1 Train: 4.596 / 0.015\n",
            "Epoch 1 Test: 4.591 / 0.028\n",
            "\n",
            "Epoch 2 Train: 4.591 / 0.020\n",
            "Epoch 2 Test: 4.586 / 0.033\n",
            "\n",
            "Epoch 3 Train: 4.585 / 0.023\n",
            "Epoch 3 Test: 4.580 / 0.037\n",
            "\n",
            "Epoch 4 Train: 4.579 / 0.026\n",
            "Epoch 4 Test: 4.573 / 0.042\n",
            "\n",
            "Epoch 5 Train: 4.572 / 0.031\n",
            "Epoch 5 Test: 4.565 / 0.044\n",
            "\n",
            "Epoch 6 Train: 4.564 / 0.035\n",
            "Epoch 6 Test: 4.556 / 0.045\n",
            "\n",
            "Epoch 7 Train: 4.555 / 0.037\n",
            "Epoch 7 Test: 4.545 / 0.046\n",
            "\n",
            "Epoch 8 Train: 4.543 / 0.040\n",
            "Epoch 8 Test: 4.531 / 0.044\n",
            "\n",
            "Epoch 9 Train: 4.530 / 0.041\n",
            "Epoch 9 Test: 4.515 / 0.045\n",
            "\n",
            "Epoch 10 Train: 4.513 / 0.041\n",
            "Epoch 10 Test: 4.497 / 0.045\n",
            "\n",
            "Epoch 11 Train: 4.495 / 0.042\n",
            "Epoch 11 Test: 4.477 / 0.045\n",
            "\n",
            "Epoch 12 Train: 4.474 / 0.044\n",
            "Epoch 12 Test: 4.454 / 0.045\n",
            "\n",
            "Epoch 13 Train: 4.451 / 0.046\n",
            "Epoch 13 Test: 4.428 / 0.048\n",
            "\n",
            "Epoch 14 Train: 4.427 / 0.046\n",
            "Epoch 14 Test: 4.400 / 0.051\n",
            "\n",
            "Epoch 15 Train: 4.399 / 0.050\n",
            "Epoch 15 Test: 4.370 / 0.054\n",
            "\n",
            "Epoch 16 Train: 4.368 / 0.053\n",
            "Epoch 16 Test: 4.338 / 0.058\n",
            "\n",
            "Epoch 17 Train: 4.338 / 0.054\n",
            "Epoch 17 Test: 4.305 / 0.060\n",
            "\n",
            "Epoch 18 Train: 4.305 / 0.056\n",
            "Epoch 18 Test: 4.271 / 0.063\n",
            "\n",
            "Epoch 19 Train: 4.271 / 0.058\n",
            "Epoch 19 Test: 4.238 / 0.066\n",
            "\n",
            "Epoch 20 Train: 4.241 / 0.062\n",
            "Epoch 20 Test: 4.206 / 0.066\n",
            "\n",
            "Epoch 21 Train: 4.210 / 0.064\n",
            "Epoch 21 Test: 4.176 / 0.070\n",
            "\n",
            "Epoch 22 Train: 4.184 / 0.065\n",
            "Epoch 22 Test: 4.149 / 0.072\n",
            "\n",
            "Epoch 23 Train: 4.158 / 0.067\n",
            "Epoch 23 Test: 4.124 / 0.073\n",
            "\n",
            "Epoch 24 Train: 4.136 / 0.069\n",
            "Epoch 24 Test: 4.102 / 0.075\n",
            "\n",
            "Epoch 25 Train: 4.117 / 0.072\n",
            "Epoch 25 Test: 4.082 / 0.078\n",
            "\n",
            "Epoch 26 Train: 4.098 / 0.073\n",
            "Epoch 26 Test: 4.063 / 0.079\n",
            "\n",
            "Epoch 27 Train: 4.080 / 0.077\n",
            "Epoch 27 Test: 4.046 / 0.082\n",
            "\n",
            "Epoch 28 Train: 4.064 / 0.080\n",
            "Epoch 28 Test: 4.030 / 0.085\n",
            "\n",
            "Epoch 29 Train: 4.048 / 0.081\n",
            "Epoch 29 Test: 4.014 / 0.087\n",
            "\n",
            "Epoch 30 Train: 4.033 / 0.084\n",
            "Epoch 30 Test: 4.000 / 0.090\n",
            "\n",
            "Epoch 31 Train: 4.019 / 0.085\n",
            "Epoch 31 Test: 3.987 / 0.091\n",
            "\n",
            "Epoch 32 Train: 4.008 / 0.087\n",
            "Epoch 32 Test: 3.974 / 0.093\n",
            "\n",
            "Epoch 33 Train: 3.994 / 0.089\n",
            "Epoch 33 Test: 3.961 / 0.096\n",
            "\n",
            "Epoch 34 Train: 3.983 / 0.090\n",
            "Epoch 34 Test: 3.950 / 0.100\n",
            "\n",
            "Epoch 35 Train: 3.969 / 0.094\n",
            "Epoch 35 Test: 3.938 / 0.099\n",
            "\n",
            "Epoch 36 Train: 3.959 / 0.095\n",
            "Epoch 36 Test: 3.926 / 0.102\n",
            "\n",
            "Epoch 37 Train: 3.947 / 0.096\n",
            "Epoch 37 Test: 3.915 / 0.107\n",
            "\n",
            "Epoch 38 Train: 3.938 / 0.099\n",
            "Epoch 38 Test: 3.904 / 0.106\n",
            "\n",
            "Epoch 39 Train: 3.923 / 0.102\n",
            "Epoch 39 Test: 3.893 / 0.110\n",
            "\n",
            "Epoch 40 Train: 3.913 / 0.101\n",
            "Epoch 40 Test: 3.883 / 0.112\n",
            "\n",
            "Epoch 41 Train: 3.907 / 0.104\n",
            "Epoch 41 Test: 3.872 / 0.112\n",
            "\n",
            "Epoch 42 Train: 3.892 / 0.106\n",
            "Epoch 42 Test: 3.861 / 0.115\n",
            "\n",
            "Epoch 43 Train: 3.883 / 0.110\n",
            "Epoch 43 Test: 3.850 / 0.118\n",
            "\n",
            "Epoch 44 Train: 3.873 / 0.109\n",
            "Epoch 44 Test: 3.840 / 0.120\n",
            "\n",
            "Epoch 45 Train: 3.862 / 0.111\n",
            "Epoch 45 Test: 3.830 / 0.122\n",
            "\n",
            "Epoch 46 Train: 3.853 / 0.112\n",
            "Epoch 46 Test: 3.821 / 0.123\n",
            "\n",
            "Epoch 47 Train: 3.839 / 0.117\n",
            "Epoch 47 Test: 3.810 / 0.126\n",
            "\n",
            "Epoch 48 Train: 3.832 / 0.116\n",
            "Epoch 48 Test: 3.802 / 0.128\n",
            "\n",
            "Epoch 49 Train: 3.822 / 0.118\n",
            "Epoch 49 Test: 3.792 / 0.129\n",
            "\n",
            "Epoch 50 Train: 3.814 / 0.120\n",
            "Epoch 50 Test: 3.784 / 0.129\n",
            "\n",
            "Epoch 51 Train: 3.804 / 0.120\n",
            "Epoch 51 Test: 3.775 / 0.130\n",
            "\n",
            "Epoch 52 Train: 3.795 / 0.122\n",
            "Epoch 52 Test: 3.767 / 0.132\n",
            "\n",
            "Epoch 53 Train: 3.785 / 0.125\n",
            "Epoch 53 Test: 3.759 / 0.135\n",
            "\n",
            "Epoch 54 Train: 3.779 / 0.127\n",
            "Epoch 54 Test: 3.752 / 0.136\n",
            "\n",
            "Epoch 55 Train: 3.771 / 0.127\n",
            "Epoch 55 Test: 3.745 / 0.137\n",
            "\n",
            "Epoch 56 Train: 3.765 / 0.127\n",
            "Epoch 56 Test: 3.737 / 0.138\n",
            "\n",
            "Epoch 57 Train: 3.753 / 0.130\n",
            "Epoch 57 Test: 3.731 / 0.139\n",
            "\n",
            "Epoch 58 Train: 3.748 / 0.131\n",
            "Epoch 58 Test: 3.725 / 0.139\n",
            "\n",
            "Epoch 59 Train: 3.741 / 0.132\n",
            "Epoch 59 Test: 3.718 / 0.141\n",
            "\n",
            "Epoch 60 Train: 3.735 / 0.133\n",
            "Epoch 60 Test: 3.712 / 0.142\n",
            "\n",
            "Epoch 61 Train: 3.728 / 0.132\n",
            "Epoch 61 Test: 3.705 / 0.143\n",
            "\n",
            "Epoch 62 Train: 3.721 / 0.135\n",
            "Epoch 62 Test: 3.700 / 0.143\n",
            "\n",
            "Epoch 63 Train: 3.712 / 0.138\n",
            "Epoch 63 Test: 3.694 / 0.143\n",
            "\n",
            "Epoch 64 Train: 3.709 / 0.137\n",
            "Epoch 64 Test: 3.689 / 0.144\n",
            "\n",
            "Epoch 65 Train: 3.703 / 0.140\n",
            "Epoch 65 Test: 3.683 / 0.146\n",
            "\n",
            "Epoch 66 Train: 3.695 / 0.139\n",
            "Epoch 66 Test: 3.678 / 0.147\n",
            "\n",
            "Epoch 67 Train: 3.690 / 0.140\n",
            "Epoch 67 Test: 3.672 / 0.147\n",
            "\n",
            "Epoch 68 Train: 3.685 / 0.142\n",
            "Epoch 68 Test: 3.667 / 0.147\n",
            "\n",
            "Epoch 69 Train: 3.681 / 0.143\n",
            "Epoch 69 Test: 3.662 / 0.148\n",
            "\n",
            "Epoch 70 Train: 3.670 / 0.145\n",
            "Epoch 70 Test: 3.658 / 0.149\n",
            "\n",
            "Epoch 71 Train: 3.668 / 0.145\n",
            "Epoch 71 Test: 3.653 / 0.149\n",
            "\n",
            "Epoch 72 Train: 3.663 / 0.146\n",
            "Epoch 72 Test: 3.647 / 0.150\n",
            "\n",
            "Epoch 73 Train: 3.657 / 0.147\n",
            "Epoch 73 Test: 3.643 / 0.151\n",
            "\n",
            "Epoch 74 Train: 3.650 / 0.148\n",
            "Epoch 74 Test: 3.638 / 0.151\n",
            "\n",
            "Epoch 75 Train: 3.648 / 0.147\n",
            "Epoch 75 Test: 3.633 / 0.154\n",
            "\n",
            "Epoch 76 Train: 3.643 / 0.149\n",
            "Epoch 76 Test: 3.629 / 0.154\n",
            "\n",
            "Epoch 77 Train: 3.637 / 0.151\n",
            "Epoch 77 Test: 3.625 / 0.154\n",
            "\n",
            "Epoch 78 Train: 3.633 / 0.151\n",
            "Epoch 78 Test: 3.620 / 0.154\n",
            "\n",
            "Epoch 79 Train: 3.627 / 0.152\n",
            "Epoch 79 Test: 3.616 / 0.155\n",
            "\n",
            "Epoch 80 Train: 3.622 / 0.154\n",
            "Epoch 80 Test: 3.612 / 0.158\n",
            "\n",
            "Epoch 81 Train: 3.615 / 0.155\n",
            "Epoch 81 Test: 3.607 / 0.158\n",
            "\n",
            "Epoch 82 Train: 3.615 / 0.153\n",
            "Epoch 82 Test: 3.603 / 0.159\n",
            "\n",
            "Epoch 83 Train: 3.609 / 0.156\n",
            "Epoch 83 Test: 3.599 / 0.160\n",
            "\n",
            "Epoch 84 Train: 3.604 / 0.156\n",
            "Epoch 84 Test: 3.595 / 0.160\n",
            "\n",
            "Epoch 85 Train: 3.598 / 0.158\n",
            "Epoch 85 Test: 3.591 / 0.159\n",
            "\n",
            "Epoch 86 Train: 3.597 / 0.156\n",
            "Epoch 86 Test: 3.587 / 0.161\n",
            "\n",
            "Epoch 87 Train: 3.588 / 0.159\n",
            "Epoch 87 Test: 3.583 / 0.161\n",
            "\n",
            "Epoch 88 Train: 3.586 / 0.159\n",
            "Epoch 88 Test: 3.579 / 0.162\n",
            "\n",
            "Epoch 89 Train: 3.581 / 0.160\n",
            "Epoch 89 Test: 3.575 / 0.162\n",
            "\n",
            "Epoch 90 Train: 3.575 / 0.161\n",
            "Epoch 90 Test: 3.571 / 0.162\n",
            "\n",
            "Epoch 91 Train: 3.570 / 0.162\n",
            "Epoch 91 Test: 3.567 / 0.163\n",
            "\n",
            "Epoch 92 Train: 3.567 / 0.162\n",
            "Epoch 92 Test: 3.563 / 0.163\n",
            "\n",
            "Epoch 93 Train: 3.562 / 0.163\n",
            "Epoch 93 Test: 3.560 / 0.165\n",
            "\n",
            "Epoch 94 Train: 3.559 / 0.162\n",
            "Epoch 94 Test: 3.555 / 0.165\n",
            "\n",
            "Epoch 95 Train: 3.552 / 0.165\n",
            "Epoch 95 Test: 3.553 / 0.165\n",
            "\n",
            "Epoch 96 Train: 3.549 / 0.166\n",
            "Epoch 96 Test: 3.548 / 0.166\n",
            "\n",
            "Epoch 97 Train: 3.544 / 0.165\n",
            "Epoch 97 Test: 3.545 / 0.166\n",
            "\n",
            "Epoch 98 Train: 3.540 / 0.165\n",
            "Epoch 98 Test: 3.542 / 0.168\n",
            "\n",
            "Epoch 99 Train: 3.536 / 0.168\n",
            "Epoch 99 Test: 3.538 / 0.167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_parameter = 0\n",
        "for parameter in model.parameters():\n",
        "    print(parameter.shape)\n",
        "    num_parameter += np.prod(parameter.size())\n",
        "print(num_parameter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BG1EcjKQPdUY",
        "outputId": "d0f31699-49ef-42dd-e571-868c7f81529d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 3072])\n",
            "torch.Size([1024])\n",
            "torch.Size([512, 1024])\n",
            "torch.Size([512])\n",
            "torch.Size([256, 512])\n",
            "torch.Size([256])\n",
            "torch.Size([100, 256])\n",
            "torch.Size([100])\n",
            "3828580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> parameter의 개수는 더럽게 많지만 성능은 LeNet에 비해 아주 안 좋음."
      ],
      "metadata": {
        "id": "EO0ur23ZPpiL"
      }
    }
  ]
}