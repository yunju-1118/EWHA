{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu6hDe9Emv/51W7d+BbN/G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunju-1118/EWHA/blob/main/seq_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN**"
      ],
      "metadata": {
        "id": "wElxc4wIN9f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "zHLllez8N9Al"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MNIST**"
      ],
      "metadata": {
        "id": "z-mjuVvYOCq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_data = MNIST(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "_,seq_len, input_size = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)"
      ],
      "metadata": {
        "id": "gY-zXcdyOV-L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = input_size*2\n",
        "\n",
        "model_name = \"rnn\""
      ],
      "metadata": {
        "id": "6iMG2jjYPHyI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN Cell** 이용"
      ],
      "metadata": {
        "id": "JWl5a8gdPMc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.RNNCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, output_shape)\n",
        "\n",
        "        def forward(self,x):\n",
        "            x = x.reshape(-1, seq_len, input_size).permute((1,0,2))\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "            for i in range(seq_len):\n",
        "                hidden_state = self.cell(x[i], hidden_state)\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            return out"
      ],
      "metadata": {
        "id": "DeueUYjlPP8G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.RNNCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, output_shape)\n",
        "\n",
        "        def forward(self,x):\n",
        "            x = x.reshape(-1, seq_len, input_size).permute((1,0,2))\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "            cell_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "            for i in range(seq_len):\n",
        "                hidden_state, cell_state = self.cell(x[i], hidden_state, cell_state)\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            return out"
      ],
      "metadata": {
        "id": "E-Q2opkoQKwQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CIFAR10**"
      ],
      "metadata": {
        "id": "1O7s_Y0AQ8pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = CIFAR10(root=path, train=True, transform=transform, download=True)\n",
        "test_data = CIFAR10(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yjkIZAjRDwf",
        "outputId": "dd1a27c1-7b95-45b9-9de5-eaebaeb68ab8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 42.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_channel, seq_len, input_size = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "hidden_size = input_size*2\n",
        "num_layers = 3\n",
        "batch_first = True ## batch를 맨 앞으로\n",
        "bidirectional = True"
      ],
      "metadata": {
        "id": "0nvOfLe9RDwf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_name == \"rnn\":\n",
        "    classifier = RNNClassifier\n",
        "elif model_name == \"lstm\":\n",
        "    classifier = LSTMClassifier\n",
        "elif model_name == \"gru\":\n",
        "    classifier = GRUClassifier"
      ],
      "metadata": {
        "id": "DiXZ24ZoRpN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self.num_channel, input_size, hidden_size, num_layers=1,\n",
        "                 batch_first=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_channel = num_channel\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bidiretional = bidirectional\n",
        "        if self.bidirectional:\n",
        "            self.direction = 2\n",
        "        else:\n",
        "            self.direction = 1\n",
        "\n",
        "        self.seq = nn.RNN(input_size=self.input_size*self.num_channel,\n",
        "                          hidden_size = self.hidden_size,\n",
        "                          num_layers = self.num_layers,\n",
        "                          batch_first = self.batch_first,\n",
        "                          bidirectional = self.bidirectional)\n",
        "        self.fc = nn.Linear(self.hidden_size*self.direction, output_shape)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.permute((0,2,3,1)).reshape(-1,seq_len,self.input_size*self.num_channel)\n",
        "        h0 = torch.zeros(self.direction*self.num_layers,batch_size, self.hidden_size)\n",
        "        out, hidden = self.seq(x,h0.detach().to(device))\n",
        "\n",
        "        out = out[:,-1,:].squeeze() # batch_size, seq_len, features\n",
        "        # bidirectional = True이면,\n",
        "        ## out = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "x8uJYaJHR6hq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}