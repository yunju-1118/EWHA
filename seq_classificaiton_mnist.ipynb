{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP8gJ0qMXt9C7eSqczA8oyH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yunju-1118/EWHA/blob/main/seq_classificaiton_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MNIST data**\n",
        "**RNN** 이용\n",
        "\n",
        "first row => first time step\n",
        "\n",
        "second row => second time step\n",
        "\n",
        ".\n",
        ".\n",
        ".\n",
        "\n",
        "28th row => 28th time step"
      ],
      "metadata": {
        "id": "YBl6EtaabmAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dZfvMCiwbNw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = MNIST(root=path, train=True, transform=transform, download=True)\n",
        "test_data = MNIST(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "_, seq_len, input_size = train_data[0][0].shape\n",
        "output_shape = len(train_data.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo5SUw2rcT9H",
        "outputId": "aedaf4e9-18b5-4de0-e9d9-be4ead744ecc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.9MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.42MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.75MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps:0')\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "else: device=torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x5yTuU-uk0L",
        "outputId": "dac8e71f-5db4-485e-b7e0-24ae0e7760c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hidden state의 input dimension을 정의해주어야 함"
      ],
      "metadata": {
        "id": "nDbmFgM-fCUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_,seq_len, input_size = train_data[0][0].shape # 1, 28, 28\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "hidden_size = input_size*2\n",
        "\n",
        "model_name = \"rnn\""
      ],
      "metadata": {
        "id": "rBLNWmGxeiU0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNNcell 이용**"
      ],
      "metadata": {
        "id": "nEGi73mVRCpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.RNNCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, output_shape)\n",
        "\n",
        "        def forward(self,x):\n",
        "            # x.shape = (100,1,28,28) (변형)=> x.squeeze() -> 28, 100, 28 = seq_len, batch_size, input_dim\n",
        "            x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2)) # 28,100,28\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "            for i in range(seq_len):\n",
        "                hidden_state = self.cell(x[i], hidden_state)\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            return out"
      ],
      "metadata": {
        "id": "AK_8-feOfYjb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RNN** => 쉽게 말해 '수열' 같음\n",
        "\n",
        "맨 처음에 존재하는 (initial) hidden state가 존재해야 남은 hidden state를 update할 수 있음\n",
        "\n",
        "=> zero vector 혹은 random한 vector로 가정"
      ],
      "metadata": {
        "id": "6-Q9yk54uL8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lstm은 cell state만 추가"
      ],
      "metadata": {
        "id": "Gzj1UFdeqTCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.LSTMCell(input_size = self.input_size,\n",
        "                                hidden_size = self.hidden_size)\n",
        "        self.fc = nn.Linear(self.hidden_size, output_shape)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2))\n",
        "            hidden_state = torch,zeros(batch_size, self.hidden_size).to(device)\n",
        "            cell_state = torch,zeros(batch_size, self.hidden_size).to(device)\n",
        "            for i in range(seq_len):\n",
        "                hidden_state, cell_state = self.call(x[i],(hidden_state, cell_state))\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            return out"
      ],
      "metadata": {
        "id": "Rcf-kSp2OXAM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = nn.GRUCell(input_size=self.input_size,\n",
        "                               hidden_size=self.hidden_size)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, output_shape)\n",
        "\n",
        "        def forward(self,x):\n",
        "            x = x.reshape(-1, seq_len, self.input_size).permute((1,0,2)) # 28,100,28\n",
        "            hidden_state = torch.zeros(batch_size, self.hidden_size).to(device)\n",
        "            for i in range(seq_len):\n",
        "                hidden_state = self.cell(x[i], hidden_state)\n",
        "            out = self.fc(hidden_state)\n",
        "\n",
        "            return out"
      ],
      "metadata": {
        "id": "Y9O60kS9Pu6t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cell state 라는 게 따로 존재하지 않아 rnn과 같음"
      ],
      "metadata": {
        "id": "Ksahx0FOQF90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if model_name == \"rnn\":\n",
        "    classifier = RNNClassifier\n",
        "elif model_name == \"lstm\":\n",
        "    classifier = LSTMClassifier\n",
        "elif model_name == \"gru\":\n",
        "    classifier = GRUClassifier"
      ],
      "metadata": {
        "id": "EBefUfQBNKb-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CIFAR10**"
      ],
      "metadata": {
        "id": "8q1DCXFARQ1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN 이용**"
      ],
      "metadata": {
        "id": "xQGB_M-pRFpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "path = './datasets/'\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_data = CIFAR10(root=path, train=True, transform=transform, download=True)\n",
        "test_data = CIFAR10(root=path, train=False, transform=transform, download=True)\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset = test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_channel, seq_len, input_size = train_data[0][0].shape  # 3*32*32\n",
        "output_shape = len(train_data.classes)\n",
        "\n",
        "hidden_size = input_size*2\n",
        "num_layers=3\n",
        "batch_first = True\n",
        "bidirectional = True\n",
        "\n",
        "model_name = \"gru\""
      ],
      "metadata": {
        "id": "54B6Ac4cRWIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if model_name == \"rnn\":\n",
        "    classifier = RNNClassifier\n",
        "elif model_name == \"lstm\":\n",
        "    classifier = LSTMClassifier\n",
        "elif model_name == \"gru\":\n",
        "    classifier = GRUClassifier"
      ],
      "metadata": {
        "id": "fz60bcB4ozwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self.num_channel, input_size, hidden_size, num_layers=1,\n",
        "                 batch_first=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_channel = num_channel\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional bidirectional\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.arrent = 2\n",
        "            else: by_direction=1\n",
        "\n",
        "        self.seq = nn.RNN(input_size = input_size*self.num_channel1,\n",
        "                          hidden_size = self.hidden_size,\n",
        "                          num_layers = self.num_layers,\n",
        "                          batch_first = self.both(bidirectional))\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size*self.direction,output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape(100, 3, 32, 32) = batch_size, num_channel, seq_len, input_dim)\n",
        "        x = x.permute((0,2,3,1)).reshape(-1, seq_len,False, _direction,out_put shape)\n",
        "        h0 = torch.zeros(self.direction*self.num_layers, batch_size, self.hidden_size)\n",
        "        out, hidden = self.seq(x,h0. detach().to(device))\n",
        "        out = out[:,-1,:].squeeze()\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Jd0WjeIIjHcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "bidirenctional = True 시,\n",
        "\n",
        "오류 발생 => 어떤 코드 수정?"
      ],
      "metadata": {
        "id": "bViJWKCjqW5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self.num_channel, input_size, hidden_size, num_layers=1,\n",
        "                 batch_first=True, bidirectional=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_channel = num_channel\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional bidirectional\n",
        "\n",
        "        if self.bidirectional:\n",
        "            self.arrent = 2\n",
        "            else: by_direction=1\n",
        "\n",
        "        self.seq = nn.LSTM(input_size = input_size*self.num_channel1,\n",
        "                          hidden_size = self.hidden_size,\n",
        "                          num_layers = self.num_layers,\n",
        "                          batch_first = self.both(bidirectional))\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size*self.direction,output_shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape(100, 3, 32, 32) = batch_size, num_channel, seq_len, input_dim)\n",
        "        x = x.permute((0,2,3,1)).reshape(-1, seq_len,False, _direction,out_put shape)\n",
        "        h0 = torch.zeros(self.direction*self.num_layers, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.direction*self,num_layers, batch_size, self.hidden_size)\n",
        "        out, hidden = self.seq(x,h0. detach().to(device))\n",
        "        out = out[:,-1,:].squeeze()\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "r2MtHN0jo3UQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}